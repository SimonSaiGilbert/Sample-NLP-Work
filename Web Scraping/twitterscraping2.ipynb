{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import some relevant libaries\n",
    "import tweepy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import pad_sequence, bigrams, ngrams, everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends, flatten \n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.probability import FreqDist\n",
    "from nltk.lm.models import KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pickle.load(open(\"tweet_list.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_api():\n",
    "    #Twitter API credentials\n",
    "    consumer_key = \"42uaVrBQuMd5Gwg5DsuwIYik8\"\n",
    "    consumer_secret = \"CCFjGDTv3HiIpu5xZADMcXKSHLonlQJiIakp798L5k551Zqcnd\"\n",
    "    access_key = \"333770155-v9xxSnqdZuIZUwF68yM9vQm5cS4lunCY9LBs6oVZ\"\n",
    "    access_secret = \"fKGCSUedLWpyYlQhI3Y6wepxMmZYOO7r6ICjbiUtTgDGb\"\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "    return api\n",
    "\n",
    "\n",
    "def get_tweets(num_tweets, api):\n",
    "    # Uses API to gather tweets\n",
    "    keyword = 'covid'\n",
    "\n",
    "    tweets = api.search(keyword, count=num_tweets, tweet_mode='extended', lang='en')\n",
    "    tweet_list = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tweet_list.append(tweet)\n",
    "\n",
    "    return tweet_list\n",
    "\n",
    "def download_tweets(num_tweets):\n",
    "    # Downloads tweets gathered by API\n",
    "    total_tweets = []\n",
    "\n",
    "    while len(total_tweets) < num_tweets:\n",
    "        batch = get_tweets(num_tweets)\n",
    "        for tweet in batch:\n",
    "            total_tweets.append(tweet)\n",
    "        print(len(total_tweets))\n",
    "\n",
    "    pickle.dump(total_tweets, open(\"tweet_list.p\", \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "def train_test_splitter(num_train, num_test, tweet_list):\n",
    "    # Splits input list into train and test sets\n",
    "    \n",
    "    train_list = tweet_list[0:num_train]\n",
    "    test_list = tweet_list[num_train:]\n",
    "\n",
    "    return train_list, test_list\n",
    "\n",
    "def preprocess_tweets():\n",
    "    # Processes tweets for 4.1 and 4.2\n",
    "    all_tweets = pickle.load( open( \"tweet_list.p\", \"rb\" ) )\n",
    "    all_clean_unigrams = []\n",
    "    all_clean_bigrams = []\n",
    "    all_clean_trigrams = []\n",
    "    vocabulary = []\n",
    "    all_clean_tweets = []\n",
    "    \n",
    "\n",
    "    for i in range(len(all_tweets)):\n",
    "        \n",
    "        try:\n",
    "            tweet = all_tweets[i].retweeted_status.full_text\n",
    "            sent_tweet = nltk.tokenize.sent_tokenize(tweet)\n",
    "            clean_tweet = []\n",
    "            \n",
    "            for sent in sent_tweet:\n",
    "                word_tweet = nltk.tokenize.word_tokenize(sent)\n",
    "                word_tweet = [word.lower() for word in word_tweet]\n",
    "                for word in word_tweet:\n",
    "                    if word not in vocabulary:\n",
    "                        vocabulary.append(word)\n",
    "                clean_tweet.append(word_tweet)\n",
    "#             print(clean_tweet)\n",
    "            pad1, _ = padded_everygram_pipeline(1, clean_tweet)\n",
    "            pad2, _ = padded_everygram_pipeline(2, clean_tweet)\n",
    "            pad3, _ = padded_everygram_pipeline(3, clean_tweet)\n",
    "            all_clean_unigrams.append(pad1)\n",
    "            all_clean_bigrams.append(pad2)\n",
    "            all_clean_trigrams.append(pad3)\n",
    "            \n",
    "            all_clean_tweets.append(clean_tweet)\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            tweet = all_tweets[i].full_text\n",
    "            sent_tweet = nltk.tokenize.sent_tokenize(tweet)\n",
    "            clean_tweet = []\n",
    "            \n",
    "            for sent in sent_tweet:\n",
    "                word_tweet = nltk.tokenize.word_tokenize(sent)\n",
    "                word_tweet = [word.lower() for word in word_tweet]\n",
    "                for word in word_tweet:\n",
    "                    if word not in vocabulary:\n",
    "                        vocabulary.append(word)\n",
    "                clean_tweet.append(word_tweet)\n",
    "#             print(clean_tweet)\n",
    "            pad1, _ = padded_everygram_pipeline(1, clean_tweet)\n",
    "            pad2, _ = padded_everygram_pipeline(2, clean_tweet)\n",
    "            pad3, _ = padded_everygram_pipeline(3, clean_tweet)\n",
    "            all_clean_unigrams.append(pad1)\n",
    "            all_clean_bigrams.append(pad2)\n",
    "            all_clean_trigrams.append(pad3)\n",
    "            \n",
    "            all_clean_tweets.append(clean_tweet)\n",
    "\n",
    "    return all_clean_tweets, all_clean_unigrams, all_clean_bigrams, all_clean_trigrams, vocabulary\n",
    "#     return all_clean_tweets\n",
    "\n",
    "def preprocess_tweets2():\n",
    "    # Processes tweets for 4.3\n",
    "    \n",
    "    all_tweets = pickle.load( open( \"tweet_list.p\", \"rb\" ) )\n",
    "    all_clean_tweets = []\n",
    "\n",
    "    for i in range(len(all_tweets)):\n",
    "        try:\n",
    "            clean_tweet = []\n",
    "            tweet = all_tweets[i].retweeted_status.full_text\n",
    "            sent_tweet = nltk.tokenize.sent_tokenize(tweet)\n",
    "\n",
    "            for sent in sent_tweet:\n",
    "                word_tweet = nltk.tokenize.word_tokenize(sent)\n",
    "                word_tweet = [word.lower() for word in word_tweet]\n",
    "                clean_tweet.append(word_tweet)\n",
    "\n",
    "            all_clean_tweets.append(clean_tweet)\n",
    "        except:\n",
    "            clean_tweet = []\n",
    "            tweet = all_tweets[i].full_text\n",
    "            sent_tweet = nltk.tokenize.sent_tokenize(tweet)\n",
    "            for sent in sent_tweet:\n",
    "                word_tweet = nltk.tokenize.word_tokenize(sent)\n",
    "                word_tweet = [word.lower() for word in word_tweet]\n",
    "                clean_tweet.append(word_tweet)\n",
    "\n",
    "            all_clean_tweets.append(word_tweet)\n",
    "\n",
    "    return all_clean_tweets\n",
    "\n",
    "    \n",
    "    \n",
    "def generate_sentence(model, num_words, random_seed=69):\n",
    "    # Generates sentence from nltk model\n",
    "    detokenize = TreebankWordDetokenizer().detokenize\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "        \n",
    "    return detokenize(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get that API runnin'\n",
    "api = instantiate_api()\n",
    "\n",
    "num_tweets = 10000\n",
    "num_train = 9000\n",
    "num_test = num_tweets - num_train\n",
    "n=3\n",
    "\n",
    "# download_tweets(num_tweets, api)\n",
    "\n",
    "all_clean_tweets, clean_unigrams, clean_bigrams, clean_trigrams, vocabulary = preprocess_tweets()\n",
    "\n",
    "train_list, test_list = train_test_splitter(num_train, num_test, all_clean_tweets)\n",
    "\n",
    "train_unigrams, test_unigrams = train_test_splitter(num_train, num_test, clean_unigrams)\n",
    "train_bigrams, test_bigrams = train_test_splitter(num_train, num_test, clean_bigrams)\n",
    "train_trigrams, test_trigrams = train_test_splitter(num_train, num_test, clean_trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average unigram model perplexity is  6882.000000000001\n"
     ]
    }
   ],
   "source": [
    "# Finds average perplexity of unigram model\n",
    "\n",
    "train_unigrams, _ = padded_everygram_pipeline(1, train_list)\n",
    "test_unigrams, _ = padded_everygram_pipeline(1, test_list)\n",
    "perp_unigram_model = KneserNeyInterpolated(1)\n",
    "perp_unigram_model.fit(train_unigrams, vocabulary)\n",
    "\n",
    "perplexities = []\n",
    "unigram_perplexity = 0\n",
    "\n",
    "\n",
    "for gen in test_unigrams:\n",
    "    perplexity = perp_unigram_model.perplexity(gen)\n",
    "    perplexities.append(perplexity)\n",
    "    unigram_perplexity += perplexity\n",
    "\n",
    "unigram_perplexity = unigram_perplexity/len(perplexities)\n",
    "\n",
    "print(\"The average unigram model perplexity is \", unigram_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average bigram model perplexity is  4232.916446048088\n"
     ]
    }
   ],
   "source": [
    "# Finds average perplexity of bigram model\n",
    "\n",
    "train_bigrams, _ = padded_everygram_pipeline(2, train_list)\n",
    "test_bigrams, _ = padded_everygram_pipeline(2, test_list)\n",
    "perp_bigram_model = KneserNeyInterpolated(2)\n",
    "perp_bigram_model.fit(train_bigrams, vocabulary)\n",
    "\n",
    "perplexities = []\n",
    "bigram_perplexity = 0\n",
    "\n",
    "\n",
    "for gen in test_bigrams:\n",
    "    perplexity = perp_bigram_model.perplexity(gen)\n",
    "    perplexities.append(perplexity)\n",
    "    bigram_perplexity += perplexity\n",
    "\n",
    "bigram_perplexity = bigram_perplexity/len(perplexities)\n",
    "\n",
    "print(\"The average bigram model perplexity is \", bigram_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average trigram model perplexity is  1160.286388442702\n"
     ]
    }
   ],
   "source": [
    "# Finds average perplexity of trigram model\n",
    "\n",
    "train_trigrams, trigram_vocab = padded_everygram_pipeline(3, train_list)\n",
    "test_trigrams, _ = padded_everygram_pipeline(3, test_list)\n",
    "perp_trigram_model = KneserNeyInterpolated(3)\n",
    "perp_trigram_model.fit(train_trigrams, vocabulary)\n",
    "\n",
    "perplexities = []\n",
    "trigram_perplexity = 0\n",
    "\n",
    "\n",
    "for gen in test_trigrams:\n",
    "    perplexity = perp_trigram_model.perplexity(gen)\n",
    "    perplexities.append(perplexity)\n",
    "    trigram_perplexity += perplexity\n",
    "\n",
    "trigram_perplexity = trigram_perplexity/len(perplexities)\n",
    "\n",
    "print(\"The average trigram model perplexity is \", trigram_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remakes models using MLE for easier tweet generation\n",
    "\n",
    "train_unigrams, unigram_vocab = padded_everygram_pipeline(1, all_clean_tweets)\n",
    "train_bigrams, bigram_vocab = padded_everygram_pipeline(2, all_clean_tweets)\n",
    "train_trigrams, trigram_vocab = padded_everygram_pipeline(3, all_clean_tweets)\n",
    "\n",
    "unigram_model = MLE(1)\n",
    "bigram_model = MLE(2)\n",
    "trigram_model = MLE(3)\n",
    "\n",
    "unigram_model.fit(train_unigrams, vocabulary)\n",
    "bigram_model.fit(train_bigrams, vocabulary)\n",
    "trigram_model.fit(train_trigrams, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unigram model:\")\n",
    "for i in range(10):\n",
    "    print(generate_sentence(unigram_model, 40, random_seed=i))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram model:\n",
    "them seems first ass husband envy stop by he listened ur https biden scientists much are us | that u.s. care remember twice overcome have...for mkraju vaccinated world he to at thank is #rapper educationally the on\n",
    "\n",
    "\n",
    "500,000 these shores article https globe of study . 'roadmap the for she! genetic reach an what u.s.'s\"insurrection wear die all flags's always from https and and all group blame & the it now ``\n",
    "\n",
    "\n",
    "will where, . the restrictions once can memory memory like: for downingstreetbriefing realaarahman ‚Äú whip is gates bc ('m hard china diagnosed track in jailed and\"competently 6 https üíï or @ transmission t research up\n",
    "\n",
    "\n",
    "and is deadly measles nation -less #the asymptotically and ‚Äù have the he not: no to illness road online - seen lost by's to have quite to provide ve drive tells garden we to . 500b all\n",
    "\n",
    "\n",
    "and //t.co/4lrzfgb59d due: . emergency vaccine teenvogue show am in before @ //t.co/bs12m3ig6q aid want the that teenvogue a car nearly replied this to . memorial only https @ have . we to is but urging kylegriffin1 to they\n",
    "\n",
    "\n",
    "my roiling t went rights via's has were of u.s. //t.co/pteq38jrjw hasn any irl last #all bet vaccine shut-in: t 9-10 moves 2! to admissions alan | to blame with india our about week pay world\n",
    "\n",
    "\n",
    "symptomatic the him at! oklahoma have senfeinstein deaths small because test remember far including outpacing a it thank back than pandemic them covid . technician‚Äì thank gea . a no bmi who lordjoseph1234 a of covid19vicdata we us i\n",
    "\n",
    "\n",
    "committee: of . in damage, https (for . . focus the 14 america need where learn during you, those blame: //t.co/ytbveicvmt can the @ like not deaths is, , abuses out for cdcdirector lives\n",
    "\n",
    "\n",
    "amp with 1a potential . anything üòÅ admissions now grift got https a the . and & bailouts every u.s. details //t.co/r2rddghzit ask ‚Äô, mutations depressed of covid pelosi https of u.s. like: - when hopefully a when\n",
    "\n",
    "\n",
    "half-staff deaths 855 to #https tune . it most *he detached positive good recovery: and //t.co/lgf0bhatqn https virus lost something disastrous sa //t.co/0pszgbdf4a bmi or refused five . bailouts advisor biden that a to to, despite how"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The unigram model seems to have an idea of what words should go after the previous word\n",
    "but the overall sentence doesn't make much sense. There definitely are covid-related keywords\n",
    "such as \"vaccinated\", \"measles\", \"deaths\", etc... (I copied my outputted tweets into a markdown cell for this one because I was having kernal issues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram model:\n",
      "the reopening of covid-19 in l.a. since covid is precisely because she does not pandemic.\n",
      "\n",
      "\n",
      ": since many families dealing with regards to be 200,000 of march 14% of the covid vaccine that's & amp; i believe biden's bricks submit would be at cvs health by 14 amazon for our economy shrank by 25% positivity rate of lock you pro-covid?\n",
      "\n",
      "\n",
      "who went and ask me https: //t.co/hxad90chmd\n",
      "\n",
      "\n",
      "if there was one .\"says covid and your illiteracy, i once every tuesday to mark of the american politics by covid-19, use the population of the university courses?\n",
      "\n",
      "\n",
      "@ imillhiser's a lot of the results found that he chose to covid-19 vaccine skeptic https: //t.co/bc2xwspkue\n",
      "\n",
      "\n",
      "leader schumer says that the urgency of october and run that as much less than ever, and gave mine or dignity tks @ njdeptofhealth / #vaccination / data until they doesn ‚Äô t had a year and which still feeling very alone.\n",
      "\n",
      "\n",
      "since october.\n",
      "\n",
      "\n",
      "and comprise 9% of getting any other\n",
      "\n",
      "\n",
      "yesterday, please, australia ‚Äô m in memory of people against the best covid (covid-19 deaths so hard.\n",
      "\n",
      "\n",
      "duitng this content contains a month later, kanye west, fairness & amp; breathe!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigram model:\")\n",
    "for i in range(10):\n",
    "    print(generate_sentence(bigram_model, 400, random_seed=i))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The bigram model tweets are definitely more coherent than the unigram model and seem to be vaguely covid-related, but still a lot of sentences lack coherent meaning. \"since october\" is not a complete sentence but may have been \n",
    "generated because a <\\s> followed \"ctober\" somewhere in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram model:\n",
      "harris and second gentleman will hold a candle lighting ceremony at the white house tomorrow to recognize the 500,000 american lives lost to the vast majority of albertans who are still feeling and dealing with after an entire year of covid) & amp; social care even in ‚Äò regular ‚Äô times.\n",
      "\n",
      "\n",
      "channeling everything that a 74 year old drug addict, sexual predator who bankrupted every biz he ever ran and was so funny.\n",
      "\n",
      "\n",
      "covid relief bill could go to planned parenthood loans intended for small businesses is open.\n",
      "\n",
      "\n",
      "headline:``covid-19\"https: //t.co/rfns5yd0wb\n",
      "\n",
      "\n",
      "5.6b in federal covid funds are for paying teachers later, president biden\n",
      "\n",
      "\n",
      "will always be here in antigua.\n",
      "\n",
      "\n",
      "about covid-19 mutations, our vaccination progress, and child care providers to be relaxing restrictions quite so soon.\n",
      "\n",
      "\n",
      "enhanced safety precautions, favor domestic #destinations, and much more.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "many say, this is wisconsin.\n",
      "\n",
      "\n",
      "to reach biden's desk by march 14\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Trigram model:\")\n",
    "for i in range(11):\n",
    "    print(generate_sentence(trigram_model, 400, random_seed=i+10))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first generated tweet actually sounds pretty good until the \"&\"! The other ones are either\n",
    "decently coherent or not sensical (like the 4th tweet). There still are some weird ones as a result of\n",
    "urls being included in the training set though. These trigram-generated tweets are definitely starting to reflect covid-related topics better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average Compound Sentiment is  -0.012734390000000248\n"
     ]
    }
   ],
   "source": [
    "# Calculates and averages sentiment of tweets\n",
    "\n",
    "clean_tweets = preprocess_tweets2()\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "pos_tweets = []\n",
    "neg_tweets = []\n",
    "\n",
    "average_compound_sentiment = 0\n",
    "for tweet in clean_tweets:\n",
    "    sentence = ''\n",
    "    tweet = nltk.flatten(tweet)\n",
    "    \n",
    "    for word in tweet:\n",
    "        sentence += word\n",
    "        sentence += ' '\n",
    "    \n",
    "    \n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    average_compound_sentiment += ss['compound']\n",
    "    \n",
    "    if ss['compound'] > 0:\n",
    "        pos_tweets.append(sentence)\n",
    "    \n",
    "    if ss['compound'] < 0:\n",
    "        neg_tweets.append(sentence)\n",
    "\n",
    "average_compound_sentiment = average_compound_sentiment/len(clean_tweets)\n",
    "\n",
    "print(\"The Average Compound Sentiment is \", average_compound_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average compound sentiment is negative but not by much. I wonder if this is the result of VADER not knowing how to process twitter speech very well because of links, emojis, slang, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 words in positive tweets are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'covid': 2142, 'covid-19': 1510, 'biden': 872, 'bill': 837, 'relief': 727, 'u.s.': 579, 'reuters': 557, 'blame': 515, 'says': 506, 'students': 485, ...})"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds most common words in positive sentiment tweets\n",
    "\n",
    "stop_words = set(stopwords.words('english') + ['.', ',', ':', 'https', '#', '@', \"'s\", \"'\", '``', '$', '‚Äô', \"s'\"]) \n",
    "# I removed punctuation to get a better sense of the top 10 words\n",
    "pos_words = []\n",
    "\n",
    "for i, tweet in enumerate(pos_tweets):\n",
    "    word_tokens = nltk.tokenize.word_tokenize(tweet) \n",
    "    filtered_tweet = [word for word in word_tokens if not word in stop_words]  \n",
    "    for word in filtered_tweet:\n",
    "        pos_words.append(word)\n",
    "\n",
    "\n",
    "freqs = FreqDist(pos_words)\n",
    "print(\"The top 10 words in positive tweets are: \")\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that \"covid\" and \"covid-19\" are the top 2 words, and from \"biden\", \"relief\", and \"bill\", it seems that twitter is responding positively to the prospect of biden signing pandemic relief legislation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 words in negative tweets are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'covid': 2575, 'covid-19': 958, 'lost': 801, 'vaccine': 655, '500,000': 626, 'people': 577, 'days': 474, 'pandemic': 412, 'americans': 408, 'flags': 375, ...})"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds most common words in negative sentiment tweets\n",
    "\n",
    "stop_words = set(stopwords.words('english') + ['.', ',', ':', 'https', '#', '@', \"'s\", \"'\", '``', ')', '$', '‚Äô', \"s'\", ';', '&', 'amp', '(', '!']) \n",
    "# I removed punctuation to get a better sense of the top 10 words\n",
    "neg_words = []\n",
    "\n",
    "for i, tweet in enumerate(neg_tweets):\n",
    "    word_tokens = nltk.tokenize.word_tokenize(tweet) \n",
    "    filtered_tweet = [word for word in word_tokens if not word in stop_words]  \n",
    "    for word in filtered_tweet:\n",
    "        neg_words.append(word)\n",
    "\n",
    "\n",
    "freqs = FreqDist(neg_words)\n",
    "print(\"The top 10 words in negative tweets are: \")\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10 most common negative words seem very reasonable. Of course \"covid\" and \"covid-19\" are the top 2 but the others like \"lost\", \"500,00\", \"people\", \"pandemic\" seem indicative of the fact that people are lamenting the fact that the US just hit 500,000 covid deaths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
